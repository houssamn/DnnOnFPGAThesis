% Chapter 1

\chapter{Introduction} % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

\section{Deep Learning and Applications}

\subsection{History}
The first artificial neural network traces back to 1958 where it was first conceived by psychologist Frank Rosenblatt \cite{perceptron}. It was called the perceptron and it was meant to model the way a human brain adapts to inputs from the external world to learn binary classification tasks. At some point someone realized that this model could be useful in pattern matching tasks. The artificial neural network is organized into layers of a single threshold logic unit that models a single neuron in the human brain. In the early days, these models were constructed physically and later on were simulated on a single computer \cite{mccarthy2006proposal}. Nowadays, learning tasks are distributed and coordinated on multiple machines to achieve a single learning task \cite{dean2012large}. The primitive perceptron developed into a structure of layers organized and separated by non-linearities. In theory, the “Universal Approximation Theorem” states that a multi-layer perceptron with one hidden layer containing a finite number of neurons can approximate any continuous function under some assumptions on the activation function \cite{csaji2001approximation}. 

\subsection{Learning in Artificial Neural Networks}
Towards the end of 1986, Hinton’s paper titled “\textit{Learning representations by back-propagating errors}”\cite{rumelhart1986learning} was published and it introduced the usefulness of an algorithm called back-propagation that can train an artificial neural network that is organized into layers. It proved more useful than the previously know perceptron-convergence algorithm \cite{widrow199030} and by the end of the 1980s many scientific institutes adopted the use of neural networks and utilized them to solve many tasks \cite{pao1989adaptive}. Unlike standard algorithms that rely on conditional procedures and hand-crafted logic, the artificial neural network if designed properly is robust to noise and can adapt to those pattern matching tasks \cite{wang1994robustness}. Artificial neural networks are exposed to thousands or millions of that are forward propagated through the weights in each of the layers. In-between each layer non-linearities are introduced and the output is compared to a specific target encoding of the output labels. From that a loss can be calculated and using a process called backpropagation \cite{rumelhart1986learning}, starting with the output layers, the network readjusts its weights to better match the target output, specifically reinforcing the connections that contribute to a correct output label.

\subsection{Deep Learning}
As computing resources were cheaper and more available and after the numerous improvements in computer hardware and architecture, scientists were able to simulate more complex networks with more neurons and deeper layers \cite{resnet}. In fact, it was even proved that deeper networks with less neurons per layer proved more useful than the shallow networks \cite{szegedy2015going}, thus the concept of deep learning was popularized. Deep learning is only a subset of the broader concept of machine learning which consists of supervised, semi-supervised, and unsupervised learning tasks. It has proven itself useful in applications related to computer vision, speech, recognition, finance, and many others. The hype over deep learning increased even more when these networks were trainable on Graphical Processing Units ( GPUs ) which are capable of performing floating point operations on hundreds and thousands of cores in parallel \cite{raina2009large}. This kind of parallelization decreased training time for these networks drastically and soon enough the suitable frameworks were developed and popularized \cite{tensorflow, theano}. Researchers were then able to utilize those hardware for training neural networks with more data and experiment with more sophisticated network models and architectures \cite{densenet, raina2009large,resnet}. 

%----------------------------------------------------------------------------------------

\section{Modern FPGAs}

\subsection{The Case for FPGAs}
The market for Field-Programmable Gate Arrays (or FPGAs)  has been increasingly growing and is expected to reach \$12.98 billion by 2023 with a compound annual growth rate of 9.0\% \footnote{\url{https://globenewswire.com/news-release/2017/11/29/1210234/0/en/Global-Field-Programmable-Gate-Array-FPGA-Market-to-Reach-USD-12-989-1-million-by-2023.html} Last Accessed : 22/08/2018 }. The demand for FPGAs was sparked by the need for high-throughput and low latency applications in industries such as aerospace, finance, and security. 
FPGAs are integrated circuits that are manufactured in a way such that they can be configured after production \cite{compton2002reconfigurable}. Using hardware descriptive languages ( HDL ) a hardware engineer can build a specific circuit and transfer it to the FPGA where it can reconfigure itself and rewire to implement a given circuit design. They offer a cheaper alternative to high-performing and specialized ASICs as they require less recurring engineering/manufacturing costs and less time to market which is necessary to thrive in this fast-paced economy. They offer a whole new dimension of customization in which complex instruction pipelines can be designed and implemented as opposed to the fixed instruction set architecture of a microcontroller or a generic CPU \cite{deepfpga}.

\subsection{FPGAs vs GPUs}
Application scientists have favored in the last couple of years the use of GPUs to accelerate deep learning tasks \cite{raina2009large}. The GPUs architecture lends itself perfectly to perform parallel floating point computations needed to compute and train the networks. Moreover, what has lead to the GPUs more widespread use is a well defined programming model and tool-sets that are easily adopted by software programmers \cite{cuda}. Dealing with those needs minimal experience in hardware architecture and applications have been parallelized and scaled massively. 
FPGAs on the other hand can offer higher power efficiency for the same computational workload as that of a GPU and are intrinsically parallel devices \cite{nurvitadhi2017can}. However it’s speed has not yet caught up with it’s accelerator counterpart. Power efficiency is a major concern for large-scale applications operating in data centers. For that we have seen most recently both Microsoft \footnote{\url{https://www.microsoft.com/en-us/research/blog/microsoft-unveils-project-brainwave/} Last Accessed: 22/08/2018} and Amazon \footnote{\url{https://aws.amazon.com/ec2/instance-types/f1/} Last Accessed: 22/08/2018} have incorporated FPGAs into their existing cloud computing infrastructure both for internal use against their data and as a service offered to clients wishing to utilize the power of reconfigurable architectures. 
Even though FPGAs up to date have only proven to be more power efficient than GPUs \cite{deepfpga, nurvitadhi2017can}, simulations and projections done at Intel Corporation predict that the upcoming generation of FPGAs will also compete with GPUs in terms of performance \cite{nurvitadhi2017can}. The projections show that the new Intel Stratix 10 is estimated to achieve 60\% higher performance and 2.3 times less power consumption than the Titan X GPU. It is also important to note that the future of deep neural networks (DNNs) have resorted to fixed point computations and lowered precision up to the point of binary values as in Binarized Neural Networks \cite{hubara2016binarized}. All of these innovations have lead to irregular types of parallelism in which FPGAs excel at compared to GPUs. GPUs can only operate on a fixed set of data types and thus the trend towards lowering precision tips the scale of performance towards FPGAs \cite{hubara2016binarized, sahin2006neuralprec, nurvitadhi2017can}. The gap in performance between FPGAs and GPUs is getting smaller and thus it is necessary to update the toolset and design workflows in designing FPGA applications to keep up and make them more accessible for developers to be able to experiment and test. 


%----------------------------------------------------------------------------------------

\section{High-Level Synthesis and OpenCL} 

\subsection{FPGA Workflow}
One of the main reasons that has lead to the slow adoption of FPGAs into commercial applications is the steep learning curve and technical background in hardware design required to be able to design and deploy applications. The usual workflow differs from that of a typical CPU application, however analogies can be drawn to bridge the gap between the two classes of applications. While designing a CPU application starts with a high-level programming language like C++ or Python, designing an FPGA circuit requires the use of hardware descriptive languages. The two most popular hardware descriptive languages are Verilog and VHDL \cite{wilson2015design}. Some FPGA design tools also offer the designer graphical user interfaces to draw schemas by dragging and dropping boxes. The latter, however, doesn’t scale for larger projects and makes it harder to collaborate within a team. HDLs are dataflow programming languages that allow for broader descriptions of how digital circuits can communicate and execute logic in ways where other procedural languages like C fall short. An FPGA application designer also has additional stages in the design cycle where behavioural simulation as well as timing simulations and functional simulations of the design. After that there is a synthesis, placement, and fitting steps in which the designer hands his design to a piece of software that performs optimizations and tries to materialize the design in terms of a binary file which can be loaded onto the FPGA. Following that the developer runs also more tests on a development board and makes sure to fix any remaining bugs or errors. Another difficulty is that FPGA designs are not portable and thus certain parameters always need to be tuned to adapt to different boards with varying configurations. Each FPGA comes with a different operating specification and different resource blocks \footnote{\url{https://www.intel.com/content/www/us/en/fpga/devices.html} Last Accessed 23/08/2018}, so in order to maximize utilization, the developer is expected to customize their design for each and every board. 

\subsection{High-Level Synthesis}
High-Level Synthesis ( HLS ) is not a new invention at all. It has always co-existed with FPGAs. From initial research into HDLs, HLS tools have advanced into C-based dataflow programming paradigms nowadays. We see the level of abstraction rising from gate level, to register-transfer level, into algorithmic level synthesis \cite{Gupta2008, hlssurv}. HLS is mainly motivated by the need to abstract hardware design to application programmers who should focus on designing and optimizing algorithms regardless of the underlying hardware. This isolation leaves hardware designers with the responsibility of optimizing intermediate representations of algorithms into synchronized logic blocks. 
Modern HLS tools begin by compiling the input specifications. Many code optimizations like code folding and dead-code elimination are carried-out to get a near-optimal input specification \cite{Gupta2008}. A control dataflow graph (CDFG) is created that parses the specification into a graph formed of nodes which are the basic blocks and connections that represent control dependencies between those blocks. The results is a register-transfer level (RTL) representation. It consists of a datapath ( memory elements, interconnects, and functional units ) and a control path. The controller is a finite-state machine that coordinates the flow of elements and operations on the datapath. The RTL representation is then verified to make sure it meets timing constraints and transformed into a gate-level representation that is then synthesized onto an FPGA according to a board specification file. 

\subsection{Xilinx and Intel} 
We have briefly motivated the main goal of HLS in abstracting the process of hardware design and mapping onto FPGAs. The two main manufacturers and technology leaders in the FPGA market are Xilinx and Intel (after the acquisition of Altera in 2015). Both companies have shifted their tool-sets to favor higher-level abstractions for synthesis but they have taken slightly different approaches. We note that Xilinx’s development workflow favors more the hardware engineer by giving more control for them to view/modify designs and control most of the nuts and bolts that transform their applications into hardware \cite{xilinxreference}. Altera’s tools however favor the software developer wishing to leverage hardware accelerators to achieve higher throughput for their application. The Intel FPGA developer is provided with a programming manual that suggests code improvement so that a better hardware design is generated. Both companies have adopted the use of HLS tools that can transform code in behavioural C/C++ and OpenCL descriptions into bitstreams \cite{xilinxreference, intel2016sdk}.

\subsection{OpenCL for FPGAs}
OpenCL\texttrademark ( Open Computing Language ) is the open standard for cross-platform parallel programming that is a subset of the C standard \cite{opencl}. OpenCL provides a programming model that fits the GPU architecture perfectly and is able to exploit parallelism through vectorized operations. GPUs are able to perform vectorized operations and have higher memory bandwidth than CPUs, enabling them to achieve high throughput by doing parallel floating point operations. The challenge in adopting OpenCL for FPGAs is being able to not only vectorize operations but to also create efficient pipelines that fully utilize the resources available on the board. For that the Intel OpenCL SDK \cite{intel2016sdk, deepfpga} performs those optimizations and allows the user to use pre-defined pragmas in order to adapt OpenCL for FPGAs. This again beats the goal for portability across platforms when different customizations have to be introduced for FPGAs, however pipeline parallelism and complex data-flow instructions prove to be an advantage and a necessary feature that should be exploited on FPGAs \cite{ddl, deepfpga}.  It is also worth noting that effort for optimizing the same OpenCL kernel differs between FPGAs and GPUs differs a lot putting FPGAs at a slight disadvantage. The reason is that with GPUs, the developer looks for the best mapping into the fixed architecture, while for FPGAs the developer guides the compiler into finding the best control and memory architecture for the given task. Moreover, compiling OpenCL kernels for FPGAs takes much longer than on GPUs as more board-specific optimizations can be done and because FPGAs allow for a broad design-exploration space.


%----------------------------------------------------------------------------------------

\section{Motivation}

This work aims to leverage the benefits of OpenCL for programming FPGAs and target implementations of modern deep neural networks. The field has gained a lot of traction and so far the support for FPGA backends for accelerating neural network computations are still research based and experimental. It is also becoming increasingly important to utilize FPGA's configurable circuits for latency-sensitive and real-time DNN applications such as autonomous driving. By using FPGAs, neural networks can be accelerated and energy efficient by utilizing pipeline parallelism. Asides from that, an additional goal is not only to create networks for inference but to also accelerate training of deep neural networks on FPGAs. For that we use the Lenet network as a case study and proof of concept and implement minibatch gradient descent for training this network. We also aim to bridge the gap between research and application, so we have created a framework in Python that is able to go from open source model definitions like ONNX into material FPGA implementations. The development framework is easily extensible with modular components that also allow for individual customization and research into novel ways of accelerating the layer computations, backpropagation, and full network pipelining as a whole. 

%----------------------------------------------------------------------------------------

\section{Outline of Next Sections}

\begin{itemize}

\item Section 2 explains the algorithms and terminology in deep learning. 

\item Section 3 explains the toolset and hardware implementation of deep neural networks on FPGAs.

\item Section 4 analyzes and discusses the results of the experiments of running the implementation on the FPGA.

\item Section 5 servers as a primer and best practices in using OpenCL for FPGAs.

\item Section 6 contains the concluding remarks and future direction of work.

\end{itemize}

%----------------------------------------------------------------------------------------

