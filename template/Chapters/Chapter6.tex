% Chapter 6

\chapter{Conclusion} % Main chapter title

\label{Chapter6} %



\section{Abstract}

Deep Learning has manifested itself into our daily lives and has become increasingly important in many pattern recognition tasks that we considered unsolvable in the past. The trends are towards deeper and more complex networks and with the availability of massive datasets, training time of neural networks increases and becomes harder to accelerate. Graphics Processing Units (GPUs) have been successfully integrated into the frameworks for deep neural networks and widely adopted. A lot of research has been done into integrating FPGAs and utilizing the soft architectures to achieve better performance with less energy consumption. In this work we bridge the gap between research and application and build a framework for translating machine learning models into efficient FPGA binaries using OpenCL. Our framework allows not only to build models, but to also verify the functionality of the neural network layers separately. 
 

\section{Conclusion}

The hype over deep learning shows no signs of slowing down. With access to largers datasets, researchers are experimenting with different architecture and the trends are generally towards deeper networks with an increasing number of parameters \citep{ddl}. Training such complex models remains a challenging task to be solved. This makes it increasingly important to leverage hardware accelerators that benefit from the inherent parallelism in the training process of neural networks. FPGAs offer a lot of advantages as opposed to other hardware accelerators. For example, pipeline parallelism and irregular forms of concurrency can be well exploited.

In this work, we made use of Intel's OpenCL SDK to build a framework for training deep neural networks. We highlighted a custom development workflow to make development using OpenCL faster and more efficient. We also build the main operators that are the basic blocks for modern neural networks on FPGAs. By using the Deep500 library, we were also able to extend the benefits of this framework to support Open Source ONNX models and allow for building kernels that perform both inference and backpropagation. We have also benchmarked different implementations of the main operators, and performed integration testing and proof of correctness of the implementation. 

The framework is still a long way from being done and a lot of work can be done to extend it. For now it serves as a proof of concept, provides the right toolset, and lays the foundation for more optimizations and operator support.

Modern implementations with lower precision have proven to work with little sacrifice on the accuracy of models, so this remains as future work to be experimented. It is challenging to port lower precision to OpenCL as fixed point types are not currently supported by the languages. However, they can still by using integer operations and bit-masking techniques. 




%----------------------------------------------------------------------------------------
