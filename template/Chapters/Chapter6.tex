% Chapter 6

\chapter{Conclusion} % Main chapter title

\label{Chapter6} %

The hype over deep learning shows no signs of slowing down. With access to largers datasets, researchers are experimenting with different architecture and the trends are generally towards deeper networks with an increasing number of parameters \citep{ddl}. Training times become longer with such complex models. This makes it increasingly important to leverage hardware accelerators that benefit from the inherent parallelism in the training process of neural networks \cite{ddl}. FPGAs offer a lot of advantages as opposed to other hardware accelerators as they introduce a large design exploration space.

In this work, we made use of Intel's OpenCL SDK \cite{intel2016sdk} to build a modular framework for training deep neural networks. We discussed our custom development workflow to make development using OpenCL faster and more efficient and also highlighted the challenges faced. We also built the main operators that are the basic blocks for modern convolutional neural networks on and simulated it on an the Altera Arria 10 GX FPGA. By using the Deep500 library, we were also able to extend the benefits of this framework to support Open Source ONNX models. Deep500 also allowed us to build a hardware implementation that performs both inference and backpropagation. We have also benchmarked different implementations of the convolution operators and discussed trade-offs. We have also performed integration testing and proof of correctness of the implementation. 

The framework is still a long way from being done and a lot of work can be done to extend it. For now it serves as a proof of concept, provides the right toolset, and lays the foundation for more optimizations and a wider range of supported layers types and operators. In practice, we have seen modern implementations with lower precision have proven to work well on FPGAs with little sacrifice on the accuracy of models \cite{gupta2015deep}. This remains  as future work to be experimented with. It is challenging to port lower precision to OpenCL as fixed point types are not currently supported by the OpenCL language, however, they can still be implemented by using integer operations and bit-masking techniques.


%----------------------------------------------------------------------------------------
